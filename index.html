<!DOCTYPE html>
<html>
<head>
  <title>People and Black Car Detector</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
</head>
<body>
  <select id="cameraSelect">
    <option value="user">Front Camera</option>
    <option value="environment">Rear Camera</option>
  </select>
  <input id="zoomInput" type="range" min="1" max="3" step="0.1" value="1">
  <video id="video" width="640" height="480" autoplay></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <script>
    // Set the initial camera options
    var constraints = { video: { facingMode: 'user', zoom: 1 } };
    // Get access to the camera
    navigator.mediaDevices.getUserMedia(constraints)
      .then(function(stream) {
        var video = document.querySelector('#video');
        video.srcObject = stream;
        video.play();
      });
    // Load the sound
    var audio = new Audio('beep.mp3');
    // Load the CocoSSD model
    var modelPromise = cocoSsd.load();
    // Detect people and black cars in the video
    setInterval(async function() {
      var model = await modelPromise;
      var predictions = await model.detect(video);
      if (predictions.some(p => p.class === 'person')) {
        audio.play();
      }
      // Draw the detection square on the canvas
      var canvas = document.querySelector('#canvas');
      var ctx = canvas.getContext('2d');
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      predictions.forEach(function(prediction) {
        ctx.beginPath();
        ctx.rect(...prediction.bbox);
        ctx.lineWidth = 3;
        ctx.strokeStyle = 'red';
        ctx.stroke();
        ctx.font = '20px Arial';
        ctx.fillStyle = 'red';
        ctx.fillText(prediction.class + ' - ' + Math.round(prediction.score * 100) + '%', prediction.bbox[0], prediction.bbox[1] - 10);
      });
    }, 400);
    // Listen for camera input change
    var cameraSelect = document.querySelector('#cameraSelect');
    cameraSelect.addEventListener('change', function(event) {
      constraints.video.facingMode = event.target.value;
      navigator.mediaDevices.getUserMedia(constraints)
        .then(function(stream) {
          var video = document.querySelector('#video');
          video.srcObject = stream;
          video.play();
        });
    });
    // Listen for zoom input change
    var zoomInput = document.querySelector('#zoomInput');
    zoomInput.addEventListener('input', function(event) {
      var zoomLevel = event.target.value;
      constraints.video.zoom = zoomLevel;
      var video = document.querySelector('#video');
      var stream = video.srcObject;
      var tracks = stream.getVideoTracks();
      tracks.forEach(function(track) {
        track.applyConstraints(constraints.video);
      });
      adjustVideoSize(zoomLevel);
    });
  // Function to adjust the size of the video element based on the zoom level
function adjustVideoSize(zoomLevel) {
  var video = document.querySelector('#video');
  var canvas = document.querySelector('#canvas');
  var videoWidth = video.videoWidth * zoomLevel;
  var videoHeight = video.videoHeight * zoomLevel;
  video.style.width = videoWidth + 'px';
  video.style.height = videoHeight + 'px';
  canvas.width = videoWidth;
  canvas.height = videoHeight;
}

// Initial adjustment of video size based on zoom level
adjustVideoSize(1);
  </script>
  </html>
